{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Linking using Knowledge-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how we approached to link the ambiguous entities using SpaCy's Knowledge-base.\n",
    "The datasets we used is the list of all the company entities from DBPedia.\n",
    "Also, we can explore different datasets in the future (Wikidata, Bloomberg, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from spacy.kb import KnowledgeBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entities():\n",
    "\n",
    "    # load the pre-processed entity dataframe from DBPedia\n",
    "    ent_df = pd.read_csv(\"ent_df.csv\")\n",
    "    \n",
    "    # create a dictionary to store info\n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "    alias = dict()\n",
    "    \n",
    "    # store each value in a dictionary\n",
    "    for idx, row in ent_df.iterrows():\n",
    "        cid = row[0]\n",
    "        name = row[1]\n",
    "        ali = row[2]\n",
    "        desc = row[3]\n",
    "        \n",
    "        # assign in ditionaries\n",
    "        names[cid] = name\n",
    "        descriptions[cid] = desc\n",
    "        \n",
    "        # make sure that alias and name are not the same\n",
    "        if name != ali:\n",
    "            alias[cid] = ali\n",
    "        \n",
    "    return names, descriptions, alias, ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_entities(kb,dic, length = 300, freq = 1):\n",
    "    \"\"\"\n",
    "        this function adds entities to the KB based on the given dictionary\n",
    "    \"\"\"\n",
    "    for qid, item in dic.items():\n",
    "        \n",
    "        # Check if the item exists in the KB\n",
    "        if len(kb.get_candidates(str(item)))>0:\n",
    "            continue\n",
    "        \n",
    "        # Create the vectorized entity. For now, it's a vector of zeros\n",
    "        item_enc = np.zeros(length)\n",
    "        \n",
    "        # insert the entities\n",
    "        kb.add_entity(entity=qid, entity_vector = item_enc, freq = freq)\n",
    "    \n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_alias(kb, dic, prob=[1]):\n",
    "    \"\"\"\n",
    "        this functions adds alias to entities in the KB based on the given dictionary\n",
    "    \"\"\"\n",
    "    for qid, ali in dic.items():\n",
    "        kb.add_alias(alias=str(ali), entities=[qid], probabilities=prob)\n",
    "        kb.add_alias(alias=str(ali.lower()), entities=[qid],  probabilities=prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entities based on the files\n",
    "name_dict, desc_dict, alias_dict, ent_df = load_entities()\n",
    "\n",
    "# Below is how we added the entities into the newly created knoweldge base.\n",
    "# Since we already have a pre-defined knowledge-base, we are going to use it.\n",
    "\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
    "\n",
    "\"\"\"\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
    "for qid, desc in desc_dict.items():\n",
    "    #desc_doc = nlp(str(desc))\n",
    "    #desc_enc = desc_doc.vector\n",
    "    desc_enc = np.zeros(300)\n",
    "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=5)\n",
    "\n",
    "# Add entities\n",
    "kb = insert_entities(kb, desc_dict)\n",
    "\n",
    "# Add aliases\n",
    "kb = insert_alias(kb, name_dict)\n",
    "\n",
    "# Add aliases\n",
    "kb = insert_alias(kb, alias_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load th pre-defined knowledge-base\n",
    "kb.load_bulk(\"kb_new\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add entities from the knowledge-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add the entities we extracted from 10-K to the knowledge-base, we first need to check if the entity is already in the KB or not. If it is not in the KB, we need to add new entity to the KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_add_kb(kb, lst):\n",
    "    \"\"\"\n",
    "        This function first checks if the given entity is in the KB.\n",
    "        If not, we add the entity to the KB.\n",
    "        Also, it addas alias to the KB for future usage\n",
    "        \n",
    "    \"\"\"\n",
    "    for item in lst:\n",
    "        if kb.get_candidates(item):\n",
    "            continue\n",
    "        else:\n",
    "            print(item)\n",
    "            kb.add_entity(entity=item, entity_vector = np.zeros(300), freq = 1)\n",
    "            kb.add_alias(alias=str(item), entities=[item], probabilities=[1])\n",
    "            kb.add_alias(alias=str(item).lower(), entities=[item], probabilities=[1])\n",
    "    \n",
    "    return kb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentences\n",
    "sent_df = pd.read_csv(\"automobile_sents.csv\")\n",
    "\n",
    "# We are goig to sue the two columns\n",
    "sent_df = sent_df[['source','target']]\n",
    "\n",
    "# removing the parantehses, / sign, and tailing white spaces\n",
    "sent_df['source'] = sent_df['source'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "sent_df['target'] = sent_df['target'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "\n",
    "# removing / sing\n",
    "sent_df['source'] = sent_df['source'].str.replace(r\"\\/.*\\/*\",\"\")\n",
    "sent_df['target'] = sent_df['target'].str.replace(r\"\\/.*\\/\",\"\")\n",
    "\n",
    "# removing 's\n",
    "sent_df['target'] = sent_df['target'].str.replace(r\"â€™s\",\"\")\n",
    "sent_df['target'] = sent_df['target'].str.replace(r\"'s\",\"\")\n",
    "\n",
    "# removing the\n",
    "sent_df['source'] = sent_df['source'].str.replace(r\"the \",\"\")\n",
    "sent_df['target'] = sent_df['target'].str.replace(r\"the \",\"\")\n",
    "\n",
    "# removing white spaces\n",
    "sent_df['source'] = sent_df['source'].str.strip()\n",
    "sent_df['target'] = sent_df['target'].str.strip()\n",
    "\n",
    "\n",
    "# get the list of unique entities\n",
    "source_list = sent_df['source'].unique()\n",
    "target_list = sent_df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the target is the same as  the source, we know that it is already in the KB\n",
    "# So we filter out only those targets that are not the same as the sources\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for idx, row in sent_df.iterrows():\n",
    "    if row['source'] != row['target']:\n",
    "        new_df = new_df.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many entities are already in the KB and how many are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick= pd.read_csv(\"quick_check.csv\", header = None)\n",
    "lst = quick[1]\n",
    "\n",
    "# Print out the entity ID if the item is in the KB\n",
    "# If not, print out the name of the company\n",
    "for item in lst:\n",
    "    if kb.get_candidates(item):\n",
    "        print(kb.get_candidates(item))\n",
    "    else:\n",
    "        print(item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
