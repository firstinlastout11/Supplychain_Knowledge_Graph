{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdHVQrqyDB50"
   },
   "source": [
    "# End text-classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the end-LSTM model for the relation extraction purpose.\n",
    "This code is originally run on Google colab.\n",
    "Thus, it might need some minor changes based on the use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the label model are the set of probabilities among the binary choice. Those probabilities(train labels) still contain noises. To achieve a high accuracy of the model, we can utilize the tokens of the sentences to train our end extraction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_T8-KVfKo2c"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import snorkel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchtext import data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BX5OGzDMKqOX",
    "outputId": "625c2756-7664-4f50-80e4-f59674542168",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text pre-processing steps require pytorch's text module called torch text.<br>\n",
    "It can simply be installed using the command \"pip install torchtext\"<br><br>\n",
    "Our inputs contain two types of text: left_text, right_text based on the location of the target entity.\n",
    "We instantiate our data objects using torchtext as below.\n",
    "\n",
    "Both left and right sentences are tokenized by word by spaCy's tokenizer <br>\n",
    "For the embedding of the text, we ustilze the glove's 300-d pretrained word-vectors. It can be downloaded easily online. Or, it can be replaced by any word-vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5-IRZl1i-1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_processing():\n",
    "\n",
    "    # Define our Torch Data objects\n",
    "    TEXT = data.Field(tokenize = \"spacy\", batch_first = True, include_lengths = True)\n",
    "\n",
    "    LABEL = data.Field(dtype = torch.float, sequential = False, use_vocab=False, batch_first = True)\n",
    "\n",
    "    # Define the fields to be used later for data-processing\n",
    "    fields = [(None, None), ('left_text',TEXT), ('right_text',TEXT), ('label',LABEL)]\n",
    "\n",
    "    # we use the dataset that contains the columns (idx, left_sentence, right_sentence, supply_label)\n",
    "    # we make the dataset into torchtext's Dataset object using the fields defined above\n",
    "    #training_data = data.TabularDataset(path = 'drive/My Drive/Data/partitioned_sents_final.csv', format='csv', fields=fields, skip_header=True)\n",
    "    training_data = data.TabularDataset(path = 'partitioned_sents_final.csv', format='csv', fields=fields, skip_header=True)\n",
    "\n",
    "    # We split the training and valid dataset (We use 80-20 ratio)\n",
    "    train_data, valid_data = training_data.split(split_ratio=0.8)\n",
    "\n",
    "    # For each sentence, we want to embed them using the pre-trained Glove vector (300-dimension)\n",
    "    #embeddings = vocab.Vectors('glove.6B.300d.txt', 'drive/My Drive/Data/')\n",
    "    embeddings = vocab.Vectors('glove.6B.300d.txt', 'glove.6B/')\n",
    "\n",
    "    # Build the vocab based on the Glove vector. Min_freq:3, Max_size=5,000\n",
    "    TEXT.build_vocab(train_data, min_freq = 3, max_size = 5000, vectors = embeddings)\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    # Store the vocab size. Note that the vocab contains extra 2 words ( <UNKNOWN>, <PAD>))\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "    \n",
    "    return TEXT, LABEL, train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT, LABEL, train_data, valid_data = text_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the End LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the end classification model, we will define a 2-layer Bi-directional LSTM model followed by a fully-connected layer for the final classification. Please note that we use the sigmoid function for the activation funcation considering that this is a binary-classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the embeddings of the two sentences, we utilize torch's pack_padded_sequence method to keep them uniform in length. Please note that those sentences shorter than the max_length will be padded with pads that are not used as inputs to the LSTM layers. <br>\n",
    "Also, the final hidden layers of both left and right sentences become concatenated and used for the final dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4Epfj1ejq6h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class SupplyClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Define the LSTM layer for each of the text using the hyparameters defined\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional,\n",
    "                           dropout = dropout, batch_first = True)\n",
    "\n",
    "        # Define the dense layer\n",
    "        # Note the input dimension : 2*2*hidden_dim because we concat left and right sentences which is Bi-drectional\n",
    "        self.fc = nn.Linear(2 * 2 * hidden_dim, output_dim)\n",
    "\n",
    "        # Define the activation function. Since it is a binary-classification, we use sigmoid\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, left_text, right_text, left_text_lengths, right_text_lengths):\n",
    "        \n",
    "        # embedded vectors for the left and right text\n",
    "        left_embedded = self.embedding(left_text)\n",
    "        right_embedded = self.embedding(right_text)\n",
    "        \n",
    "        \"\"\" We use the pack_padded_sequence to make all the sentence unform in the length\n",
    "            Sentences that are shorter than text_lengths are filled with <pad>\n",
    "            Note that <pad> is not used as a part of inputs to the LSTM model\n",
    "            By setting batch_first = True, the inputs have the shape [batch size, sentence length, embedding dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        left_packed_emb = pack_padded_sequence(left_embedded, left_text_lengths, batch_first = True, enforce_sorted=False)\n",
    "        right_packed_emb = pack_padded_sequence(right_embedded, right_text_lengths, batch_first = True, enforce_sorted=False)\n",
    "        \n",
    "        \n",
    "        # we store the outputs, hidden_states and cell_states for each of the sentence\n",
    "        left_out, (l_hidden, l_cell) = self.lstm(left_packed_emb)\n",
    "        right_out, (r_hidden, r_cell) = self.lstm(right_packed_emb)\n",
    "        \n",
    "        # Since our model is Bi-LSTM, we need to concatenate the hidden states for each direction\n",
    "        l_hidden = torch.cat((l_hidden[-2,:,:], l_hidden[-1,:,:]), dim = 1)\n",
    "        r_hidden = torch.cat((r_hidden[-2,:,:], r_hidden[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Finally, we concatenate the hidden states for both left and right sentence\n",
    "        hidden = torch.cat((l_hidden, r_hidden), dim = 1)\n",
    "        \n",
    "        # We input the concatenated hidden states into out Fully-connected layer\n",
    "        fc_out = self.fc(hidden)\n",
    "        \n",
    "        # Then, we acquire the final results by putting the fc_out into our sigmoid activation function\n",
    "        final = self.act(fc_out)\n",
    "        \n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9fCA2gWjtDQ"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "        This function computes the binary accuracy.\n",
    "    \"\"\"\n",
    "    # round the preds to 0 or 1\n",
    "    preds = torch.round(preds)\n",
    "    \n",
    "    # check if the preds are correct\n",
    "    check = (preds == y).float()\n",
    "    \n",
    "    # accuracy\n",
    "    acc = check.sum() / len(check)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "hidden_nodes = 32\n",
    "output_nodes = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate  the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters and hyper-parameters defined above, we instantiate the model.  <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PL6u_sYYjuYd"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# instantiate the end-model\n",
    "model = SupplyClassifier(vocab_size = vocab_size,\n",
    "                        embedding_dim = embedding_dim,\n",
    "                        hidden_dim = hidden_nodes,\n",
    "                        output_dim = output_nodes,\n",
    "                        n_layers = num_layers,\n",
    "                        bidirectional = bidirectional,\n",
    "                        dropout = dropout)\n",
    "\n",
    "# use GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Store the pre-trained embeddings for each word and input into our model\n",
    "gloves = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(gloves)\n",
    "\n",
    "# Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define the optimizer. We use ADAM.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the loss function. We use BCELoss since it is a binary classification\n",
    "loss_criterion = nn.BCELoss()\n",
    "loss_criterion = loss_criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TC2UKOhpjvuy"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_criterion):\n",
    "    \"\"\"\n",
    "        This model trains the end model given the iterator, optimizer and loss_criterion\n",
    "    \"\"\"\n",
    "    # set the accuracy and loss to zero every iteration\n",
    "    loss_epoch = 0\n",
    "    acc_epoch = 0\n",
    "    \n",
    "    # Initialize the training phase\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # for each batch, store the text and the length of the sentences\n",
    "        left_text, left_text_len = batch.left_text\n",
    "        right_text, right_text_len = batch.right_text\n",
    "\n",
    "        # flatten to 1-D\n",
    "        preds = model(left_text, right_text, left_text_len, right_text_len).squeeze()\n",
    "\n",
    "        # compute the loss of the batch\n",
    "        loss = loss_criterion(preds, batch.label.squeeze()) \n",
    "\n",
    "        # compute the accuracy for the batch\n",
    "        acc = binary_accuracy(preds, batch.label)\n",
    "\n",
    "        # perform back-prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate the loss and accuracy\n",
    "        loss_epoch += loss.item()\n",
    "        acc_epoch += acc.item()\n",
    "    return loss_epoch / len(iterator), acc_epoch / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFb3qTRFjxGj"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_criterion):\n",
    "    \n",
    "    # set the accuracy and loss to zero every iteration\n",
    "    loss_epoch = 0\n",
    "    acc_epoch = 0\n",
    "    \n",
    "    #Initialize the evaluation phase\n",
    "    model.eval()\n",
    "    \n",
    "    # We don't need to record the grads for this evaluation process\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in iterator:\n",
    "            \n",
    "            # for each batch, store the text and the length of the sentences\n",
    "            left_text, left_text_len = batch.left_text\n",
    "            right_text, right_text_len = batch.right_text            \n",
    "            # flatten to 1-D\n",
    "            preds = model(left_text, right_text, left_text_len, right_text_len).squeeze(1)\n",
    "\n",
    "            # compute the loss of the batch\n",
    "            loss = loss_criterion(preds, batch.label)\n",
    "          \n",
    "            # compute the accuracy for the batch\n",
    "            acc = binary_accuracy(preds, batch.label)\n",
    "\n",
    "            # accumulate the loss and accuracy\n",
    "            loss_epoch += loss.item()\n",
    "            acc_epoch += acc.item()\n",
    "            \n",
    "    return loss_epoch / len(iterator), acc_epoch / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz9Qp6mUjyO6"
   },
   "outputs": [],
   "source": [
    "# Define the train and valid iterator using BucketIterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_key = lambda x: len(x.left_text), # Sort the batches by text length size\n",
    "        sort_within_batch = True,\n",
    "        device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "colab_type": "code",
    "id": "WewhJB9MjziH",
    "outputId": "06722e53-363e-4922-b852-b2faa3c963b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.046 | Train Acc: 98.31%\n",
      "Epoch 1 Validation Loss: 0.021 |  Val. Acc: 99.30%\n",
      "Epoch 2 Train Loss: 0.016 | Train Acc: 99.52%\n",
      "Epoch 2 Validation Loss: 0.015 |  Val. Acc: 99.52%\n",
      "Epoch 3 Train Loss: 0.009 | Train Acc: 99.73%\n",
      "Epoch 3 Validation Loss: 0.013 |  Val. Acc: 99.62%\n",
      "Epoch 4 Train Loss: 0.006 | Train Acc: 99.84%\n",
      "Epoch 4 Validation Loss: 0.015 |  Val. Acc: 99.60%\n",
      "Epoch 5 Train Loss: 0.004 | Train Acc: 99.89%\n",
      "Epoch 5 Validation Loss: 0.021 |  Val. Acc: 99.62%\n",
      "Epoch 6 Train Loss: 0.003 | Train Acc: 99.92%\n",
      "Epoch 6 Validation Loss: 0.025 |  Val. Acc: 99.62%\n",
      "Epoch 7 Train Loss: 0.002 | Train Acc: 99.94%\n",
      "Epoch 7 Validation Loss: 0.027 |  Val. Acc: 99.58%\n",
      "Epoch 8 Train Loss: 0.002 | Train Acc: 99.96%\n",
      "Epoch 8 Validation Loss: 0.035 |  Val. Acc: 99.59%\n",
      "Epoch 9 Train Loss: 0.002 | Train Acc: 99.96%\n",
      "Epoch 9 Validation Loss: 0.033 |  Val. Acc: 99.62%\n",
      "Epoch 10 Train Loss: 0.001 | Train Acc: 99.96%\n",
      "Epoch 10 Validation Loss: 0.038 |  Val. Acc: 99.57%\n",
      "Epoch 11 Train Loss: 0.001 | Train Acc: 99.97%\n",
      "Epoch 11 Validation Loss: 0.037 |  Val. Acc: 99.62%\n",
      "Epoch 12 Train Loss: 0.001 | Train Acc: 99.97%\n",
      "Epoch 12 Validation Loss: 0.036 |  Val. Acc: 99.61%\n",
      "Epoch 13 Train Loss: 0.001 | Train Acc: 99.97%\n",
      "Epoch 13 Validation Loss: 0.041 |  Val. Acc: 99.61%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-306-8b9d05b8d82c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-303-6127c7c935de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, loss_criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# perform back-prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_criterion)\n",
    "    \n",
    "    # evaluate the model\n",
    "    val_loss, val_acc = evaluate(model, valid_iterator, loss_criterion)\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        \n",
    "    print(f'Epoch {epoch + 1} Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'Epoch {epoch + 1} Validation Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYmmOTpRVpQQ"
   },
   "outputs": [],
   "source": [
    "#load weights\n",
    "path='/content/saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict(model, df):\n",
    "\n",
    "    result = []\n",
    "    probs_result = []\n",
    "    for idx, row in df.iterrows():\n",
    "        left_sent = row['left_sents']\n",
    "        right_sent = row['right_sents']\n",
    "\n",
    "        left_tokenized = [tok.text for tok in nlp.tokenizer(left_sent)]\n",
    "        right_tokenized = [tok.text for tok in nlp.tokenizer(right_sent)]\n",
    "\n",
    "        left_indexed = [TEXT.vocab.stoi[t] for t in left_tokenized]\n",
    "        right_indexed = [TEXT.vocab.stoi[t] for t in right_tokenized]\n",
    "\n",
    "        l_length = [len(left_indexed)] \n",
    "        r_length = [len(right_indexed)] \n",
    "\n",
    "        l_tensor = torch.LongTensor(left_indexed).to(device)              #convert to tensor\n",
    "        l_tensor = l_tensor.unsqueeze(1).T \n",
    "\n",
    "        r_tensor = torch.LongTensor(right_indexed).to(device)\n",
    "        r_tensor = r_tensor.unsqueeze(1).T\n",
    "\n",
    "        l_len = torch.LongTensor(l_length)\n",
    "        r_len = torch.LongTensor(r_length)\n",
    "\n",
    "        prediction = model(l_tensor, r_tensor, l_len, r_len)\n",
    "        probs_result.append([1 - prediction.item(), prediction.item()])\n",
    "        result.append(round(prediction.item()))\n",
    "    return result, probs_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the train dataset to evaluate our trained-classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "58Qp0HN5qrsi",
    "outputId": "22c2de2d-823f-42c5-ba5e-2c0e0d3fcd98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label model accuracy score: 0.8821331521739131\n",
      "Label model f1 score: 0.593200468933177\n",
      "Label model roc-auc score: 0.9381681937395687\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "# Load the test-set\n",
    "df = pd.read_csv(\"drive/My Drive/Data/test_dataset.csv\", index_col = 0)\n",
    "df['left_sents'] = df['left_sents'].astype('str')\n",
    "df['right_sents'] = df['right_sents'].astype('str')\n",
    "df['supply'] = df['supply'].astype('float')\n",
    "\n",
    "\n",
    "# compute the answers and probability of the test-set\n",
    "ans, probs = predict(model, df)\n",
    "print(\n",
    "      f\"Label model accuracy score: {metric_score(df['supply'], np.array(ans), metric='accuracy')}\"\n",
    "\n",
    ")\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(df['supply'], np.array(ans), metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model roc-auc score: {metric_score(df['supply'], probs = np.array(probs), metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgVfBlvZDEja"
   },
   "source": [
    "# GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "rxlkxvkrCyin",
    "outputId": "99916b4e-f42b-42c4-f932-fd6a698ccccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 16 10:39:27 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyDODoaWC6KI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Colab_25GBRAM_GPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
