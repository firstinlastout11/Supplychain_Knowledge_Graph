{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.matcher import Matcher \n",
    "import spacy\n",
    "import snorkel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    LSTM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction based on Weak-Supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to extract supply-chain relations using weak-supervision concepts.\n",
    "The datasets are the text extracted from 10-K on SEC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load the Spacy vocab core\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tuple(df):\n",
    "    \"\"\"\n",
    "        This function serves to change the position tuple into a list\n",
    "    \"\"\"\n",
    "    tups = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        temp = []\n",
    "\n",
    "        for tok in row['position'].split(\", \"):\n",
    "            num = int(tok.replace(\"(\", \"\").replace(\")\", \"\")) \n",
    "            temp.append(num) \n",
    "        \n",
    "        tups.append(temp)\n",
    "    return tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        This function loads the data and replaces some values\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    df = pd.read_csv(\"final_merged.csv\")\n",
    "    df = df[['Column','source','target_x','supply','sentence','position']]\n",
    "    df = df.rename(columns={'target_x':'target'})\n",
    "    df = df[df['position'].notnull()]\n",
    "    \n",
    "    # change string into tuples for the position value \n",
    "    tups_lst = string_to_tuple(df)\n",
    "    df['position'] = tups_lst\n",
    "\n",
    "    # replace string to numbers\n",
    "    df['supply'] = df['supply'].replace('0',0)\n",
    "    df['supply'] = df['supply'].replace('0.0',0)\n",
    "    df['supply'] = df['supply'].replace('1.0',1)\n",
    "    df['supply'] = df['supply'].replace('1',1)\n",
    "    df['supply'] = df['supply'].replace('?',0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df):\n",
    "    \"\"\"\n",
    "        This function pre-processes the loaded data.\n",
    "        For each given sentence, we create a list of tokens. \n",
    "        Then, based on the position of the named entity(company name), we extract the left and right tokens\n",
    "        Then, it splits the data into dev, train, and test set as pandas dataframes.\n",
    "    \"\"\"\n",
    "    # Initiate new lists to store the pre-processed values\n",
    "    tokens_lst = []\n",
    "    left_tokens_lst = []\n",
    "    right_tokens_lst = []\n",
    "\n",
    "    # Data pre-processing\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # change the sentence into spacy's object\n",
    "        doc = nlp(row['sentence'])\n",
    "\n",
    "        # token list of the sentence        \n",
    "        toks = [tok.orth_ for tok in doc]\n",
    "\n",
    "        # store position\n",
    "        start, end = row['position']\n",
    "\n",
    "        # append the values to the lists\n",
    "        tokens_lst.append(toks)\n",
    "        left_tokens_lst.append(toks[:start])\n",
    "        right_tokens_lst.append(toks[end+1:])\n",
    "\n",
    "    # Assign those computed lists into the datafamr\n",
    "    df['tokens'] = tokens_lst\n",
    "    df['left_tokens'] = left_tokens_lst\n",
    "    df['right_tokens'] = right_tokens_lst\n",
    "\n",
    "    # split the dataframe based on the labels: 1, 0, unknown\n",
    "    df_zero = df[df['supply'] == 0]\n",
    "    df_one = df[df['supply'] == 1]\n",
    "    df_null = df[df['supply'].isnull()]\n",
    "\n",
    "    # types into integers\n",
    "    df_zero['supply'] = df_zero['supply'].astype('int64')\n",
    "    df_one['supply'] = df_one['supply'].astype('int64')\n",
    "\n",
    "    # unlabeled data become training set\n",
    "    df_train = df_null[['source','target','sentence','position','tokens','left_tokens','right_tokens']]\n",
    "\n",
    "    # creating the dataframes\n",
    "    X_one = df_one[['source','target','sentence','position','tokens','left_tokens','right_tokens']]\n",
    "    Y_one = np.array(df_one['supply'])\n",
    "\n",
    "\n",
    "    X_zero = df_zero[['source','target','sentence','position','tokens','left_tokens','right_tokens']]\n",
    "    Y_zero = np.array(df_zero['supply'])\n",
    "\n",
    "\n",
    "    # split the labeled dataframe into dev and test set\n",
    "    X_one_val, X_one_test, Y_one_val, Y_one_test = train_test_split(X_one, Y_one, test_size = 0.5)\n",
    "    X_zero_val, X_zero_test, Y_zero_val, Y_zero_test = train_test_split(X_zero, Y_zero, test_size = 0.5)\n",
    "\n",
    "    # concatenate the 1, 0 labeled data\n",
    "    df_dev = pd.concat([X_one_val,X_zero_val])\n",
    "    Y_dev = np.append(Y_one_val,Y_zero_val)\n",
    "\n",
    "    df_test = pd.concat([X_one_test,X_zero_test])\n",
    "    Y_test = np.append(Y_one_test,Y_zero_test)\n",
    "\n",
    "    return df_dev, Y_dev, df_train, df_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'final_merged.csv' does not exist: b'final_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de98e50d0bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pre-process the data and split then into dev, train, and test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dd45f9b05fd2>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_merged.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Column'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target_x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'supply'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'target_x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'final_merged.csv' does not exist: b'final_merged.csv'"
     ]
    }
   ],
   "source": [
    "# we load the data\n",
    "df = load_data()\n",
    "\n",
    "# pre-process the data and split then into dev, train, and test set\n",
    "df_dev, Y_dev, df_train, df_test, Y_test = data_preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak-supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined different labeling functions to extract supply-chain relations from the text.\n",
    "Please note that those labeling functions can be re-used to extract other relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "# Possible outputs for labeling function. \n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "# Check for the `spouse` words appearing between the person mentions\n",
    "supplying = {\"supplier\", \"supplied\",  \"supplying\", \"supplies\", \"supply\"}\n",
    "@labeling_function(resources=dict(supplying=supplying))\n",
    "def lf_supply(row, supplying):\n",
    "    for term in supplying:\n",
    "        if term in row['sentence']:\n",
    "             return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "customer = {\"customers\",\"customer\"}\n",
    "@labeling_function(resources=dict(customer=customer))\n",
    "def lf_customer(row, customer):\n",
    "    for term in customer:\n",
    "        if term in row['sentence']:\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "sales_to = {\"sales to\"}\n",
    "@labeling_function(resources=dict(sales_to=sales_to))\n",
    "def lf_sales_to(row, sales_to):\n",
    "    for term in sales_to:\n",
    "        if term in row['sentence']:\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "our_customer = {\"our\", \"customers\"}\n",
    "@labeling_function(resources=dict(our_customer=our_customer))\n",
    "def lf_our_customer(row, our_customer):\n",
    "    if \"our\" in row['sentence'] and \"customers\" in row['sentence']:\n",
    "        return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "acquisition= {\"acquisition\", \"acquired\"}\n",
    "@labeling_function(resources=dict(acquisition=acquisition))\n",
    "def lf_acquisition(row, acquisition):\n",
    "    for term in acquisition:\n",
    "        if term in row['sentence']:\n",
    "            return NEGATIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "people = {\"CEO\",'ceo','manager','Manager','Mr.','Mrs.','Ms.'}\n",
    "@labeling_function(resources=dict(people=people))\n",
    "def lf_people(row, people):\n",
    "    for term in people:\n",
    "        if term in row['sentence']:\n",
    "            return NEGATIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "sold = {\"sold to\"}\n",
    "@labeling_function(resources=dict(sold=sold))\n",
    "def lf_sold(row, sold):\n",
    "    for term in sold:\n",
    "        if term in row['sentence']:\n",
    "            return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "relations = {\"relationship\",\"with\"}\n",
    "@labeling_function(resources=dict(relations=relations))\n",
    "def lf_relation(row, relations):\n",
    "    if \"relation\" in row['sentence'] and \"with\" in row['sentence']:\n",
    "        return POSITIVE\n",
    "    return ABSTAIN\n",
    "\n",
    "competition = {\"competitors\",\"competition\"}\n",
    "@labeling_function(resources=dict(competition=competition))\n",
    "def lf_competition(row, competition):\n",
    "    for term in competition:\n",
    "        if term in row['sentence']:\n",
    "            return NEGATIVE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lf_supply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b7cb1c541154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Accumulate all the labeling_functions for supply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m supply_lfs = [\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlf_supply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlf_customer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlf_sales_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lf_supply' is not defined"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "# Accumulate all the labeling_functions for supply\n",
    "supply_lfs = [\n",
    "    lf_supply,\n",
    "    lf_customer,\n",
    "    lf_sales_to,\n",
    "    lf_our_customer,\n",
    "    lf_acquisition,\n",
    "    lf_people,\n",
    "    lf_sold,\n",
    "    lf_relation,\n",
    "    lf_competition\n",
    "]\n",
    "\n",
    "# Apply the above labeling functions to the data in Pandas dataframe formats\n",
    "applier = PandasLFApplier(supply_lfs)\n",
    "\n",
    "# Use the applier of the labeling functions to both development set and train set\n",
    "L_dev = applier.apply(df_dev)\n",
    "L_train = applier.apply(df_train)\n",
    "\n",
    "# Analyze the performance of the labeling functions.\n",
    "# Our development set had hand-labeled labels so we can check the accuracies\n",
    "\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   78.0%\n",
      "Label Model Accuracy:     90.4%\n",
      "Label model f1 score: 0.6293333333333333\n",
      "Label model roc-auc: 0.940890357300998\n"
     ]
    }
   ],
   "source": [
    "# Baseline model: Majority voting among all the labeling functions\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "majority = MajorityLabelVoter()\n",
    "preds_train = majority.predict(L=L_train)\n",
    "\n",
    "\"\"\" \n",
    "    Our model: Snorkel Label Model\n",
    "    This model considers the probabilistic aspects of the labeling functions\n",
    "    It produces a single set of noise-aware labels\n",
    "\"\"\"\n",
    "\n",
    "# caridnality : 2 (True and False)\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "# Fit the label_model\n",
    "label_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500)\n",
    "\n",
    "# accruacy for the majority model using the test set\n",
    "majority_acc = majority.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "# accuracy for the label model using the test set\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")\n",
    "      \n",
    "# check the F-1 score and ROC_AUC score\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Label model roc-auc: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the extraction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the label model are the set of probabilities among the binary choice. Those probabilities(train labels) still contain noises. To achieve a high accuracy of the model, we can utilize the tokens of the sentences to train our end extraction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we filter out those data points that did not receive any labels from any of the labelinng function to minimize the potential noises. Then, we will train a bidrectional LSTM model with the train data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To train the extraction model, \n",
    "we first output the probabilities of the binary choices: True and False from our label model.\n",
    "Then, using the probabilities, we train our end model\n",
    "\"\"\"\n",
    "\n",
    "# extract the probabiliteis from the training set using our label model\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "\n",
    "# Since we cannot use the data points that did not receive any labels (Not covered by our labeling functions),\n",
    "# we filter them out\n",
    "\n",
    "# extract only the data points that received any labels from the labeling functions\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=probs_train, L=L_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "#def uniform_length(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "def uniform_length(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\" \n",
    "        since the length of sentence varies much, we make the lengths uniform\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract three: tokens, left_tokens, right_tokens\n",
    "    toks = df.tokens\n",
    "    left_toks = df.left_tokens\n",
    "    right_toks = df.right_tokens\n",
    "    \n",
    "    \n",
    "    def token_filter(l, max_len=50):\n",
    "        return l[:max_len] + [\"\"] * (max_len - len(l))\n",
    "\n",
    "    tokens = np.array(list(map(token_filter, toks)))\n",
    "    left_tokens = np.array(list(map(token_filter, left_toks)))\n",
    "    right_tokens = np.array(list(map(token_filter, right_toks)))\n",
    "    \n",
    "    return left_tokens, right_tokens\n",
    "    #return tokens, left_tokens, right_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm(tokens: tf.Tensor, rnn_state_size: int = 64, num_buckets: int = 40000, embed_dim: int = 36,):\n",
    "    \"\"\"\n",
    "        Bidirectional LSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts each string in the input Tensor to its hash mod by a number of buckets.\n",
    "    ids = tf.strings.to_hash_bucket_fast(tokens, num_buckets)\n",
    "    \n",
    "    # Turns positive integers (indexes) into dense vectors of fixed size\n",
    "    embedded_input = Embedding(num_buckets, embed_dim)(ids)\n",
    "    \n",
    "    # return the bidrecitonal LSTM\n",
    "    return Bidirectional(LSTM(rnn_state_size, activation=tf.nn.relu))(\n",
    "        embedded_input, mask=tf.strings.length(tokens)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(\n",
    "    rnn_state_size: int = 64, num_buckets: int = 40000, embed_dim: int = 12) -> tf.keras.Model:\n",
    "    \n",
    "    \"\"\"\n",
    "    This lstm model predicts the label probailities given the embedded tokens\n",
    "    \n",
    "    rnn_state_size: state size of LSTM model\n",
    "    num_buckets: Number of buckets to hash strings to integers\n",
    "    embed_dim: Size of token embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    #toks_ph = Input((None,), dtype=\"string\")\n",
    "    #toks_embs = bidirectional_lstm(b_ph, rnn_state_size, num_buckets, embed_dim\n",
    "    #layer = Concatenate(1)([left_embs, bet_embs, right_embs])\n",
    "    \n",
    "    # Instantiate Input Keras Object. Data type : string\n",
    "    left_obj = Input((None,), dtype=\"string\")\n",
    "    right_obj = Input((None,), dtype=\"string\")\n",
    "    \n",
    "    # intput embeddings\n",
    "    left_lstm = bidirectional_lstm(left_obj, rnn_state_size, num_buckets, embed_dim)\n",
    "    right_lstm = bidirectional_lstm(right_obj, rnn_state_size, num_buckets, embed_dim)\n",
    "    \n",
    "    # concatenate two inputs\n",
    "    layer = Concatenate(1)([left_lstm, right_lstm])\n",
    "    \n",
    "    # Dense layers with relu activations\n",
    "    layer = Dense(64, activation=tf.nn.relu)(layer)\n",
    "    layer = Dense(32, activation=tf.nn.relu)(layer)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    probabilities = Dense(2, activation=tf.nn.softmax)(layer)\n",
    "    \n",
    "    #  final model using the characteristics above\n",
    "    model = tf.keras.Model(inputs=[left_obj, right_obj], outputs=probabilities)\n",
    "    \n",
    "    #model = tf.keras.Model(inputs=[bet_ph, left_ph, right_ph], outputs=probabilities)\n",
    "    \n",
    "    # compile the model: AdagradOptimizer, cross_entropy\n",
    "    model.compile(tf.train.AdagradOptimizer(0.1), \"categorical_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3141/3141 [==============================] - 45s 14ms/sample - loss: 0.6828\n",
      "Epoch 2/50\n",
      "3141/3141 [==============================] - 27s 9ms/sample - loss: 0.6807\n",
      "Epoch 3/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.6784\n",
      "Epoch 4/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.6746\n",
      "Epoch 5/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.6710\n",
      "Epoch 6/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.6680\n",
      "Epoch 7/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.6509\n",
      "Epoch 8/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.6304\n",
      "Epoch 9/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.5845\n",
      "Epoch 10/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.5390\n",
      "Epoch 11/50\n",
      "3141/3141 [==============================] - 22s 7ms/sample - loss: 0.4981\n",
      "Epoch 12/50\n",
      "3141/3141 [==============================] - 22s 7ms/sample - loss: 0.4721\n",
      "Epoch 13/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.4428\n",
      "Epoch 14/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.4317\n",
      "Epoch 15/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.4206\n",
      "Epoch 16/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.4084\n",
      "Epoch 17/50\n",
      "3141/3141 [==============================] - 27s 9ms/sample - loss: 0.4065\n",
      "Epoch 18/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3902\n",
      "Epoch 19/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3905\n",
      "Epoch 20/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3832\n",
      "Epoch 21/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3791\n",
      "Epoch 22/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3727\n",
      "Epoch 23/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3732\n",
      "Epoch 24/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3664\n",
      "Epoch 25/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3600\n",
      "Epoch 26/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3558\n",
      "Epoch 27/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3749\n",
      "Epoch 28/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3532\n",
      "Epoch 29/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3507\n",
      "Epoch 30/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3496\n",
      "Epoch 31/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3464\n",
      "Epoch 32/50\n",
      "3141/3141 [==============================] - 27s 9ms/sample - loss: 0.3453\n",
      "Epoch 33/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.3463\n",
      "Epoch 34/50\n",
      "3141/3141 [==============================] - 27s 9ms/sample - loss: 0.3452\n",
      "Epoch 35/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.3422\n",
      "Epoch 36/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3426\n",
      "Epoch 37/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3410\n",
      "Epoch 38/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3381\n",
      "Epoch 39/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3407\n",
      "Epoch 40/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3389\n",
      "Epoch 41/50\n",
      "3141/3141 [==============================] - 27s 8ms/sample - loss: 0.3391\n",
      "Epoch 42/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3397\n",
      "Epoch 43/50\n",
      "3141/3141 [==============================] - 23s 7ms/sample - loss: 0.3360\n",
      "Epoch 44/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3368\n",
      "Epoch 45/50\n",
      "3141/3141 [==============================] - 25s 8ms/sample - loss: 0.3355\n",
      "Epoch 46/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3353\n",
      "Epoch 47/50\n",
      "3141/3141 [==============================] - 24s 8ms/sample - loss: 0.3397\n",
      "Epoch 48/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.3346\n",
      "Epoch 49/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.3391\n",
      "Epoch 50/50\n",
      "3141/3141 [==============================] - 26s 8ms/sample - loss: 0.3337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aff1b4ef0>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = uniform_length(df_train_filtered)\n",
    "model = rnn_model()\n",
    "batch_size = 64\n",
    "model.fit(X_train, probs_train_filtered, batch_size=batch_size, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtting the model, we evaluate our results using the test set. Note that the test set is fairly unbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy when trained with soft labels: 0.8053668478260869\n",
      "Test F1 when trained with soft labels: 0.4609595484477893\n",
      "Test ROC-AUC when trained with soft labels: 0.9074948056813925\n"
     ]
    }
   ],
   "source": [
    "X_test = uniform_length(df_test)\n",
    "probs_test = model.predict(X_test)\n",
    "preds_test = probs_to_preds(probs_test)\n",
    "\n",
    "print(\n",
    "    f\"Test accuracy when trained with soft labels: {metric_score(Y_test, preds=preds_test, metric='accuracy')}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Test F1 when trained with soft labels: {metric_score(Y_test, preds=preds_test, metric='f1')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test ROC-AUC when trained with soft labels: {metric_score(Y_test, probs=probs_test, metric='roc_auc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what sentences did our model label as supply relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>sentence</th>\n",
       "      <th>supply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Motivnation, Inc.</td>\n",
       "      <td>TrixMotive</td>\n",
       "      <td>On December 7, 2005, a customer of TrixMotive filed a lawsuit in the Superior Court of Santa Clara County of California against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Motivnation, Inc.</td>\n",
       "      <td>TrixMotive</td>\n",
       "      <td>On December 7, 2005, a customer of TrixMotive filed a lawsuit in the Superior Court of Santa Clara County of California against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Motivnation, Inc.</td>\n",
       "      <td>TrixMotive</td>\n",
       "      <td>On January 24, 2008, a customer of TrixMotive filed a lawsuit in the Superior Court of Middlesex County of New Jersey against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Motivnation, Inc.</td>\n",
       "      <td>TrixMotive</td>\n",
       "      <td>On January 24, 2008, a customer of TrixMotive filed a lawsuit in the Superior Court of Middlesex County of New Jersey against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Stanadyne Corp</td>\n",
       "      <td>John Deere</td>\n",
       "      <td>Deere was the only customer that accounted for more than 10% of Stanadyne’s net sales in 2012 and 2011 , at 41.4% , and 38.3% , respectively.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38846</th>\n",
       "      <td>Remy International, Inc.</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>Net sales to our other largest customer, Hyundai, accounted for approximately 12% , 10% , 9% , and 9% of our net sales for the years ended December 31, 2014 and 2013 , the period August 15, 2012 to December 31, 2012 and the period January 1, 2012 to August 14, 2012 , respectively.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38849</th>\n",
       "      <td>Remy International, Inc.</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>Hyundai is our fastest growing OEM customer.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38897</th>\n",
       "      <td>Remy International, Inc.</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>In 2014, Hyundai became our largest customer and accounted for approximately 12% and 10% of our net sales for the years ended December 31, 2014 and 2013 , respectively.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38898</th>\n",
       "      <td>Remy International, Inc.</td>\n",
       "      <td>General Motors Co</td>\n",
       "      <td>GM, our second largest customer, accounted for 12% and 16% of our net sales for 2014 and 2013 , respectively.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39036</th>\n",
       "      <td>Remy International, Inc.</td>\n",
       "      <td>VIA</td>\n",
       "      <td>Net sales to VIA were $1,285,000 and $198,000 during the years ended December 31, 2014 and 2013 , respectively.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1112 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         source             target  \\\n",
       "31     Motivnation, Inc.         TrixMotive          \n",
       "32     Motivnation, Inc.         TrixMotive          \n",
       "36     Motivnation, Inc.         TrixMotive          \n",
       "37     Motivnation, Inc.         TrixMotive          \n",
       "160    Stanadyne Corp            John Deere          \n",
       "...               ...                   ...          \n",
       "38846  Remy International, Inc.  Hyundai             \n",
       "38849  Remy International, Inc.  Hyundai             \n",
       "38897  Remy International, Inc.  Hyundai             \n",
       "38898  Remy International, Inc.  General Motors Co   \n",
       "39036  Remy International, Inc.  VIA                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                        sentence  \\\n",
       "31     On December 7, 2005, a customer of TrixMotive filed a lawsuit in the Superior Court of Santa Clara County of California against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.                             \n",
       "32     On December 7, 2005, a customer of TrixMotive filed a lawsuit in the Superior Court of Santa Clara County of California against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.                             \n",
       "36     On January 24, 2008, a customer of TrixMotive filed a lawsuit in the Superior Court of Middlesex County of New Jersey against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.                               \n",
       "37     On January 24, 2008, a customer of TrixMotive filed a lawsuit in the Superior Court of Middlesex County of New Jersey against TrixMotive claiming for breach of contract and warranty, intentional and negligence misrepresentation for a customized vehicle.                               \n",
       "160    Deere was the only customer that accounted for more than 10% of Stanadyne’s net sales in 2012 and 2011 , at 41.4% , and 38.3% , respectively.                                                                                                                                               \n",
       "...                                                                                                                                              ...                                                                                                                                               \n",
       "38846  Net sales to our other largest customer, Hyundai, accounted for approximately 12% , 10% , 9% , and 9% of our net sales for the years ended December 31, 2014 and 2013 , the period August 15, 2012 to December 31, 2012 and the period January 1, 2012 to August 14, 2012 , respectively.   \n",
       "38849  Hyundai is our fastest growing OEM customer.                                                                                                                                                                                                                                                \n",
       "38897  In 2014, Hyundai became our largest customer and accounted for approximately 12% and 10% of our net sales for the years ended December 31, 2014 and 2013 , respectively.                                                                                                                    \n",
       "38898  GM, our second largest customer, accounted for 12% and 16% of our net sales for 2014 and 2013 , respectively.                                                                                                                                                                               \n",
       "39036  Net sales to VIA were $1,285,000 and $198,000 during the years ended December 31, 2014 and 2013 , respectively.                                                                                                                                                                             \n",
       "\n",
       "       supply  \n",
       "31     1       \n",
       "32     1       \n",
       "36     1       \n",
       "37     1       \n",
       "160    1       \n",
       "...   ..       \n",
       "38846  1       \n",
       "38849  1       \n",
       "38897  1       \n",
       "38898  1       \n",
       "39036  1       \n",
       "\n",
       "[1112 rows x 4 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the probabilities of the train set\n",
    "sp = model.predict(X_train)\n",
    "\n",
    "# outputs\n",
    "sp_label = probs_to_preds(sp)\n",
    "\n",
    "# create a temp df\n",
    "temp_train = df_train_filtered[['source','target','sentence']]\n",
    "temp_train['supply'] = sp_label\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "temp_train[temp_train['supply'] == 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
